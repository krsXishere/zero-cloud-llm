# Quick Start Guide - JavaScript Version

## âš¡ Fastest Way to Get NPU Acceleration

### 1. Install Dependencies

```bash
npm install
```

### 2. Run the Application

**Interactive mode:**

```bash
npm start
```

**Demo mode:**

```bash
node src/main.js --demo
```

**Single prompt:**

```bash
node src/main.js --prompt "What is AI?"
```

### 3. First Run

The model will download automatically (~1GB):

```
ğŸ”„ Loading model...
âš ï¸  First run will download the model (~1GB). Please wait...
```

Wait for:

```
âœ… Model loaded successfully!
ğŸš€ NPU acceleration active (if available on your hardware)
```

### 4. Test It

```bash
npm test
```

This will:

- Load the DeepSeek model
- Run a math equation prompt
- Show streaming output
- Verify everything works

## ğŸ¯ Expected Output

```
============================================================
ZERO CLOUD LLM - NPU Accelerated Inference
============================================================
Model: onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX
Quantization: q4f16
Max Tokens: 512
============================================================

ğŸ”„ Loading model...
âœ… Model loaded successfully!
ğŸš€ NPU acceleration active (if available on your hardware)

============================================================
INTERACTIVE MODE
============================================================
Commands:
  - Type your prompt and press Enter
  - Type 'quit' or 'exit' to exit
  - Type 'clear' to clear screen
============================================================

ğŸ‘¤ You: _
```

## ğŸ” Verify NPU Usage

1. **Open Task Manager** (Windows)
2. **Run the app** and send a prompt
3. **Watch CPU usage**:
   - With NPU: CPU stays low (<20%)
   - Without NPU: CPU spikes (50-100%)

## ğŸš€ Why JavaScript Version is Better

### Python Version

âŒ Complex setup (ONNX export, providers, etc.)  
âŒ Manual model conversion needed  
âŒ NPU provider configuration  
âŒ More code to write

### JavaScript Version

âœ… One command: `npm install`  
âœ… Auto-downloads ONNX models  
âœ… Auto-detects NPU  
âœ… Built-in streaming  
âœ… Works out of the box

## ğŸ“ Example Usage

```javascript
import { pipeline, TextStreamer } from "@huggingface/transformers";

// That's it! NPU is automatically used if available
const generator = await pipeline(
  "text-generation",
  "onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX",
  { dtype: "q4f16" }
);

const messages = [{ role: "user", content: "Hello!" }];
const output = await generator(messages, { max_new_tokens: 100 });
```

## ğŸ¨ Try Different Models

Edit `src/main.js`:

```javascript
// Smaller & faster
model: "onnx-community/Qwen2.5-0.5B-Instruct";

// Better quality
model: "onnx-community/Phi-3-mini-4k-instruct-onnx";

// Original (default)
model: "onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX";
```

## ğŸ› Common Issues

**"Cannot find module '@huggingface/transformers'"**

```bash
npm install
```

**Out of memory**

```javascript
// Use smaller model
model: "onnx-community/Qwen2.5-0.5B-Instruct";
```

**Slow on first run**

- Model is downloading (~1GB)
- Subsequent runs will be instant

## ğŸ‰ That's It!

You now have a working NPU-accelerated LLM running locally!

No complex ONNX exports, no provider configuration, just works! ğŸš€
