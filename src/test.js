/**
 * Simple test script for Zero Cloud LLM
 */

import { pipeline, TextStreamer } from "@huggingface/transformers";

console.log("ðŸ§ª Testing ONNX DeepSeek Model...\n");

// Create a text generation pipeline
console.log("ðŸ“¦ Loading model...");
const generator = await pipeline(
    "text-generation",
    "onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX",
    { dtype: "q4f16" }
);

console.log("âœ… Model loaded!\n");

// Define the test message
const messages = [
    { role: "user", content: "Solve the equation: x^2 - 3x + 2 = 0" },
];

console.log("ðŸ¤– Generating response...\n");

// Create text streamer
const streamer = new TextStreamer(generator.tokenizer, {
    skip_prompt: true,
});

// Generate a response
const output = await generator(messages, {
    max_new_tokens: 512,
    do_sample: false,
    streamer,
});

console.log("\n\nâœ… Test completed!");
console.log("ðŸ“„ Full response:", output[0].generated_text.at(-1).content);
